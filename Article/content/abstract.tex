\begin{abstract}
    Facial Emotion Recognition (FER) is a crucial and complex task in computer vision with applications across numerous fields such as human-computer interaction, psychology, healthcare, and marketing. FER aims to accurately detect and interpret a personâ€™s emotional state based on facial expressions. However, significant challenges arise due to the inherent variability of expressions across different individuals, cultural backgrounds, and contexts. While recent advances in deep learning, particularly in Convolutional Neural Networks (CNNs), have significantly improved the accuracy of FER systems, these models still struggle to generalize well across diverse datasets, especially when facial emotions are subtle or mixed.
    
    This paper introduces a novel architecture for 3D multimodal facial expression recognition, which leverages a combination of CNNs for feature extraction, a spatial attention network applied in cross modality and a transformer encoder for contextual representation. By integrating these modalities, the proposed system effectively captures spatial features from facial expressions, addressing challenges related to subtle emotions. To validate the proposed approach, extensive experiments were conducted on a widely recognized dataset (BU3DFE), and a novel dataset (CalD3rMenD3s). A final experiment is conducted on a merged dataset which is currently the largest static dataset for 3D facial expression recognition in the literature.

    The proposed architecture demonstrates the potential of multimodal architectures in advancing the field of FER showing competitive performance compared to existing methods.
 
 \end{abstract}