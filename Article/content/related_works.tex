\section{Related Works and challenges}
Savchenko et al. \cite{RW_7_EFF1}\cite{RW_7_EFF2} developed multi-task, CNN based models using architectures like MobileNet \cite{MobileNetv1} and EfficientNet \cite{EfficientNet} and RexNet \cite{RexNet}. They demonstrated such lightweighted and fast backbones are enough to achieve state of the art performances in a multitude of tasks (face verification, face recognition and FER) providing a strong baseline for many applications. Such multitask networks are usually made of multiple streams (one or more per sub-task) and they compute a final loss by combining the losses of the individual subtask (usually with a sum).
In particular, in \cite{RW_7_EFF2}, they pre-train different lightweight CNNs over VGGFace2 \cite{VGGFace2} (for face recognition) and, in a second experiment, over ImageNet \cite{ImageNet} datasets (for object recognition). The pretraining is useful to let the network produce features suitable to discriminate subjects from another. 
The backbone is appended with 4 stream networks (one per subtask) and finetuned over AffectNet \cite{AffectNet}. These 4 stream networks are FC layers for gender, age, ethnicity recognition subtasks, while the one for FER subtask also containes some additional convolutional layer. This is because FER is a different task from attribute recognition, so a common approach is to extract earlier features from the backbone CNN and fine-tune the convolutional layers for the downstream task (FER). They show that pretrained backbone over VGGFace2 leads to better accuracies (\textasciitilde +4\%) in FER task over AffectNet with respect to the same backbone pretrained over ImageNet. This is because VGGface2 is a dataset specifically designed for face analysis tasks, so the backbone CNN will learn to extract more relevant features for such tasks.\\

 Another CNN based work is DDAMFN by Zhang et al. \cite{RW_11_DDAMFN}. Similarly to Savchenko et al. they decide to use lightweight CNN backbone as a feature extractor (MobileFaceNet \cite{MobileFaceNet}) enhanced for FER task. The common idea raises from the fact that using deeper architectures on small FER tasks could lead to overfitting.\\
The first enhancement consists in applying multiple-size kernels that permit to capture more diverse features from input images, similarly to the Inception Block from GoogleNet \cite{Inception_block}. The second enhancement is the Coordinate Block Attention Module (CBAM)\cite{CBAM} which is used to capture the long-range spatial dependencies between different regions of the feature maps. The CAM is composed of two branches to capture the vertical dependencies and the horizontal dependencies respectively. The two branches are then combined to produce the final attention map which is summed to initial feature map.\\
 
Ning et al. in 2024 \cite{RW_12A_FMAE} produced FMAE wich is a Masked Autoencoder (MAE) from \cite{RW_12B_MAE} trained over Face9M dataset. Face9M is ad hoc created dataset containing 9M images from the unification of common facial datasets used for face analysis tasks (CelebA \cite{CelebA}, FFHQ \cite{FFHQ}, VGGFace2 \cite{VGGFace2}, CASIA-WebFace \cite{CASIA-WebFace}, UMDFaces \cite{UMDFaces}, MegaFace \cite{MegaFace}, LAION-5B\cite{LAION-5B}, EDFace-Celeb-1M \cite{EDFace-Celeb-1M}). 
As with any autoencoder, MAE is composed of an encoder and a decoder. The encoder maps input image into a latent feature of smaller dimension; while the decoder reconstructs the input from the latent feature. Intuitevely, pre-training the encoder in this self-supervised setting allows it to learn features that are relevant for reconstruction, embedding meaningful information for the classification task. After pre-training, the decoder is discarded, and the encoder is used as a feature extractor.\\

In MAE, the encoder is encouraged to learn useful features by applying random patches to the input image. By omitting some parts of the input image, the encoder is forced to learn feature embeddings invariant to the masked regions. This is particularly useful in computer vision in general, because, differently from language data, image data are not so information-dense and contain a lot of redundancy \cite{RW_12B_MAE}.
Note that this masking idea acts as a regularization (applied at pre-training time) and is very common in FER world to train models more robust to occlusions.\cite{RW_12A_FMAE} 
Therefore, FMAE \cite{RW_12A_FMAE} pre-trains a Large ViT \cite{Vision_Transformer} over Face9M dataset with MAE approach and then use it for FER (and landmark detection) task.Note that FMAE is the best performing model on AffectNet \cite{AffectNet} and RAF-DB \cite{RAFDB} only using this pre-training and without any complex network architecture or particular attention mechanism. Of course this comes at the high cost of pre-training a large model over a huge dataset.\\

 Generally speaking, using a CNN backbone for feature extraction and a subsequent attention module to learn relationships from different components in the extracted feature map is a very common strategy in FER. Recently many of these attention modules are becoming transformer based, because they show better ability in evaluating the global context information of the image than CNNs which are limited by the receptive field. This technique, efficiently overcomes the CNNs limitations in capturing long-range dependencies, but comes at the cost of longer training times and larger datasets since learning complex relationships in sequence-like data with transformers is expensive.\\

For example, in 2023, Mao et al., proposed POSTERV2 \cite{RW_1_POSTERV2}, a transformer-based model that achieves state of the art performances on RAF-DB and AffectNet. The model is composed of two backbones, the first one extracting features from RGB images (a pre-trained IR-50 \cite{IR-50} that is fine-tuned) and second one that extracts features from landmark information (a pre-trained MobileFaceNet \cite{MobileFaceNet} used frozen (non fine-tuned)). The two features are then fused with a transformer used in cross attention setup, which is a transformer that formulates queries using the input from the other modality (e.g., $Q_{RGB}= X_{landmark} \cdot W_{Q_{RGB}} $). This is done 3 times in sequence to extract low, mid and high level features; which are concatenated and fed into a final lightweight ViT \cite{Vision_Transformer} (only 2 layers) to learn relationships in the feature embedding and perform classification.\\

A very similar approach is used by Her et al. in 2024 \cite{RW_13_BTN}. They use the same basic architecture with the two stream approach (pre-trained IR-50 for RGB  and pre-trained MobileFaceNe for landmarks) with cross attention modules and the final ViT; but they address the annotation ambiguity problem by applying Class Batch Normalization (CBN). According to them, even few "noisy" (badly annotated) samples can significantly degrade FER performance \cite{RW_13_BTN} leading to overfitting, so CBN is used. CBN is a variant of Batch Normalization that normalizes the features of each class separately, rather than the entire batch. In this way they assure that the final features are more class-specific, mitigating the noise from some sample in the batch that may be badly annotated, but also, addressing the high intra-class variance problem.\\

Another similar model is ARBEX \cite{RW_5_ARBEX} from Wasi et al. that use again the dual stream network, cross attention modules and final ViT.\\

Xue at al. in TransFER\cite{RW_4_TRANSFER} point out the fact that landmark based approaches, like POSTERV2, lack of robustness in wild datasets where landmarks may be not visible because of occlusions, insufficient light or even head pose (for example if the subjects is near profile pose). Another problem is the scaling of the image, as it could be too small to precisely detect landmarks. Finally, the subject's characteristics, such as age and ethnicity, could also be obstacles to consistent landmark detection. Large variations in wrinkles, fat distribution, and muscle tone can make it more difficult to identify key points consistently across different age groups.\\
TransFER uses a stem CNN (pretrained IR-50) to extract feature from the image. This base feature is then processed by many spatial attention networks which apply 2 $1\times1$ convolutional layers reducing the number of channels to 1. These masks are then Maxpooled, multiplied by original feature map and fed to a transformer encoder to learn relationships between them.\\
The innovation is in addressing overfitting through a dropout-like strategy. More in depth, one random mask is dropped with probability p1 and also one random head of the transformer is dropped with probability p2, at each forward pass. This strategy is used to prevent the model from overfitting to specific regions of the face and to improve generalization ability.\\

Huang et al. uses two attention mechanisms in FERVT \cite{RW_3_FERVT}. The first is a grid-wise attention mechanism that allows the model to extract features focusing on different areas of the face image. The second mechanism is a vision transfomer fed with these low-level features which captures long-range dependencies .\\

As another example of transformers on top of CNNs, Ma et al. produced VTFF (Visual Transformers with Feature Fusion) \cite{RW_6_VTFF} which proposes a fusion strategy for RGB and LBP features (extracted from two backbone ResNet18 \cite{ResNet}) in a global-local attention mechanism that helps the model to be more robust to occlusion and pose variations. More in depth, the features extracted from the two backbones are initially multiplied by two learnable matrixes and then summed. This is done to let the network learn, initially, how much to account for each modality. The overall feature is then processed through a global attention stream (that reduces spatial dimension to $1\times1$ by Average pooling) and through a local attention stream (which instead retains spatial information). Finally, the attended global and local features are summed and later processed through a large transformer to learn inner relationships between feature components and perform classification. Note that, VTFF actually reaches lower accuracies than other state of the art methods, but the global-local attention express its power when, in original paper, they manually add occlusions to the images demostrating that VTFF is more robust to occlusions than other models.\\

Li et al. introduced FER-former \cite{RW_2_FER-former} which is a transformer based model for FER in the wild. Again they try to combine features provided by CNNs with transformers in an hybrid model. The innovation in this work is that, inspired by OpenAI CLIP model \cite{RW_4B_CLIP}, they address annotation ambiguity by using soft labels instead of the hard labels which are believed to be noisy. This is particularly true for FER datasets, where annotation ambiguity is usually addressed with a majority voting system, which is very expensive (many expert annotators must be involved).\\
FER-former comprises a stem feature extractor (they try both ViT and IR-50). These features are then linearly projected and fed to a transformer encoder. In a parallel stream, the soft labels are fed into the pretrained Text encoder from CLIP (which is a transformer trained on image-text pairs from the internet) to extract context aware features. "Soft labels" refers to labels that embed the original one-hot label but are also able to give context information to the text encoder (e.g., formulations like "This is a face image of \{expression\} " or passive sentences like "a/an \{expression\} expression is shown in the image").\\
Later the cosine similarity between the image feature $I$ and the text feature $T$ is computed and used as loss. In this way, the image feature extractor is trained to produce image features that are close to the text features produced from the text encoder, thereby easing the issue of annotation ambiguity.\\
At test time, the text encoder is discarded and the image feature extractor is used as a standard FER model.\\

Multimodal emotion recognition leverages information from multiple modalities to enhance the accuracy and robustness of emotion recognition systems. By combining information from different sources, multimodal systems can capture a more comprehensive view of human emotions, leading to more accurate and reliable emotion recognition. The most common modalities used in multimodal emotion recognition are RGB images and 3D facial scans. In particular, 3D facial scans have gained popularity due to their ability to capture detailed facial geometry and expression dynamics, making them well-suited for capturing subtle emotional cues that may be missed in 2D images. 3D representations offer a more robust description that is invariant to environmental variations irrelevant
to the FER task (such as illumination or occlusion variations), but they come at higher collection cost.\\ The most common 3D representation are \textbf{depth maps} which are 2D images that encode the depth information of the face. Other 3D representations like \textbf{meshes} and \textbf{point clouds} are not very popular because of the high computational cost associated with their processing due to their unordered structure. Instead, depth maps are commonly used as they provide a good trade-off between accuracy and computational cost since they can be easily processed by 2D convolutional neural networks. For example, \cite{RW_8A_AFNET}, \cite{RW_8B_CMANET} extract depth maps from 3D meshes and use them as input to convolutional neural networks.
When it comes to multimodal networks, the most common approach is to use a \textbf{feature concatenation} (or sum) strategies where the features extracted from each modality are concatenated (or summed) and fed to a classifier. However, this approach has some limitations, such as the inability to capture complex interactions between modalities and the risk of overfitting due to the high dimensionality of the concatenated features. To address these challenges, some works have explored strategies  where features from different modalities are combined at different stages to produce a final embedding containing information from all modalities at early, mid and late stages (e.g., \cite{RW_1_POSTERV2}, \cite{RW_11_DDAMFN}) allowing for more effective integration of information across modalities. However this approach produces very high dimensional features that are difficult to furtherly process for classification. Another approach, which will be of inspiration for this work, is to use \textbf{cross-modal attention mechanisms} (e.g., \cite{RW_6_VTFF}), which enable the model to focus on the most relevant information from each modality, improving the overall performance of the system. These approaches are particularly effective in capturing complex interactions between modalities and use strengh of the information from each modality to improve the performance of the system.\\

Transformer based models are much more rare for 2D+3D FER because, as already mentioned, transformers generally need more data and there are no large-scale 2D+3D FER datasets due to the high collection costs. This is why most of the works in this field are based on CNNs. To overcome this, Li et al. in MFEVIT\cite{RW_8D_MFEVIT} in 2021, tried to use a lightweight vision transformers.
The same authors later abandoned transformers (probably because prone to overfitting considering the very small sized datasets in 2D+3D FER) in AFNET \cite{RW_8A_AFNET}, CMANET \cite{RW_8B_CMANET} and FFNET \cite{RW_8C_FFNET}. They opted instead for ad-hoc developed cross-modality attention fusion networks which are less "data-hungry" and require less training than transformer.\\
A common idea used in these works is the application of manually forged masks. Masks are images used as prior knowledge to enhance the CNNs feature extraction. Basically, the network will learn to extract features by focusing on the salient regions indicated by the masks\cite{RW_8A_AFNET} (e.g., eyes, nose, and mouth). The drawback is that, these attention masks can only be effective if all images in the dataset are aligned and in the same format, making it unfeasible for real-world collected dataset or spontaneous dataset where the head rotation may change. In the ablation studies, they get a very small decrease of performance when they drop the depth modality because they use only BU3DFE \cite{BU3DFE} and Bosphorus \cite{Bosphorus} as benchmark datasets, which are lab acquired datasets not containing much variation in occlusions, pose and illumination, therefore it is reasonable that the network learns to give much more weight to the RGB modality which is more discriminant in these conditions.\\

A different approach to solve 2D+3D FER is proposed by Ni et al. in CMFN \cite{RW_9_CMFN}. Exactly as in above cited works using Bosphorus and BU3DFE datasets, the depth maps are extracted from the 3D meshes, but they convert RGB images into gray scale images and then extract LBP images because LBP features are hand-crafted to be representative of texture information and less affected by illumination changes than RGB. So RGB images are grayscaled to avoid redundant information and let the network focus on the LBP stream.\\
They extract features from the three modalities (RGB, depth maps and LBP) with specifically developed lightweight CNNs and feed them into a cross-modal fusion network that combines them through attention mechanism similar to the AFNET, CMANET and FFNET. 

These attention based fusion networks used in 2D+3D FER perform better than concatenation or sum fusion because, each modality may inherently emphasize different aspects of the facial expression and it is important to let the network learn how much to account each modality for. As mentioned, 2D images capture surface texture and color variations in the face, while depth maps are better at representing subtle facial geometry. So, it is crucial to consider the complementarity of each modality when combining them, such that the model can compensate for weaknesses in one modality with strengths from another. Moreover, it reduces the risk for one modality to dominate the fused feature overshadowing the valuable information provided by the other modality.\\

Whether multimodal or "full RGB", most of the above cited works address the problem of high intra-class variance and high inter-class similarity trough the use of attention mechanisms (specifically developed or transformer based) and use a final standard classification mechanism with softmax and CE loss. Another approach is to make use of loss functions that enhance the discriminative power of the features. For example, Wen et al. in \cite{center_loss} implement center loss to extract well separated and compact features for a face recognition task over the LFW \cite{LFW} and YTF \cite{YTF} datasets. CMFN \cite{RW_9_CMFN} addresses class unmbalance problem using a focal loss.\\ Also ARBEX \cite{RW_5_ARBEX} uses complex loss function summing the CE loss, Central Loss and an anchor based loss. Lin et al. in \cite{RW_10_OGFNET} use an orthogonalization loss.\\


Current state of the art solutions, even if presenting similarities, seem to not converge to a unique general solution for FER, but, rather, to a set of solutions specifically designed for different datasets characteristics. For example embedding prior knowledge by using manually forged masks like in \cite{RW_8A_AFNET} works only for the spefific dataset at hand where all images are aligned. In general, more data leads to better generalization, so, despite all the efforts in building robust and efficient architectures, FER will be solved when enough data will be available. For example, the ViT pretrained in a self-supervised manner over 9 milions images (Face9M) dataset in FMAE \cite{RW_12A_FMAE} is able to reach state of the art performances without any particular architecture.\\

In conclusion, the most promising direction seems to be the use of transformers, but, as already mentioned, they are very data-hungry and require large datasets to be trained. This is why, in this work multiple datasets are merged to try overcome this limitation.

\subsection{Challenges in FER}
FER faces several significant challenges that impact its accuracy and effectiveness. Addressing these challenges is essential to improve FER systems, especially given the growing demand for robust applications in real-world environments.

One of the primary challenges is the lack of sufficiently diverse and exhaustive training data. Current state-of-the-art models for FER make use of transformers (e.g., \cite{RW_1_POSTERV2}\cite{RW_6_VTFF}\cite{RW_4_TRANSFER} and many others) which are very "data-hungry" and require large, high-quality, and diverse datasets to achieve high levels of generalization. However, most available FER datasets are limited in diversity and size, leading to \textbf{overfitting}. Overfitting occurs when models memorize the training data, including noise and outliers, but fail to generalize to new, unseen data. To mitigate this, techniques such as data augmentation and the merging of multiple datasets are employed \cite{RW_12A_FMAE}. However, merging datasets is not straightforward, as they often vary in annotation types, collection methodologies, and image quality. Furthermore, spontaneous datasets frequently exhibit imbalanced classes due to the inability of some subjects to adequately display all emotions. Solutions to this issue include adjusting loss functions to handle imbalance and downsampling overrepresented classes during training (e.g. \cite{RW_8B_CMANET} using Focal Loss).

Another critical issue is \textbf{annotation ambiguity}, which arises due to the inherent subjectivity of emotions. Human emotions can be difficult to define with precision, leading to inconsistent labeling, either due to the ambiguous nature of emotional expressions or human error during the annotation process. This annotation noise can significantly impact model performance, as models trained on inconsistent labels may learn non-generalizable patterns . To address this, some datasets rely on multiple annotators, using majority voting to determine the final label \cite{AffectNet} \cite{FER+}. However, even this approach cannot fully eliminate the subjectivity of emotional interpretation, meaning that FER systems must learn to operate under inherent uncertainty. Some approaches involve class specific batch normalization \cite{RW_4_TRANSFER} or the use of soft-labels \cite{RW_13_BTN} to handle annotation ambiguity.

\textbf{Inter-class similarity} is another common challenge in FER, where certain facial expressions share similar features across different emotion categories. For example, frowning eyebrows may be associated with multiple emotions such as anger, disgust, or contempt. Similarly, emotions like fear and surprise may both involve wide-open eyes, leading to potential confusion for models attempting to distinguish between these categories. 

\textbf{Intra-class variability} presents another significant challenge, particularly in datasets collected in uncontrolled environments. Intra-class variability refers to the wide range of variations within the same emotion class, stemming from differences in how individuals express emotions based on factors such as age, ethnicity, gender, and overall expressiveness. Additionally, variations in head pose, lighting conditions, and partial occlusion of facial features can further complicate emotion recognition in unconstrained scenarios. 

Addressing \textit{inter-class similarity} and \textit{Intra-class variability} requires models capable of capturing subtle differences between similar expressions, which often involves carefully designed feature extraction techniques and the use of advanced loss function that allow for production of more discriminant features. For example, the \textbf{Island Loss} \cite{island_loss} is a popular loss function for learning embeddings that are more discriminative by pushing samples of the same class closer together and samples of different classes further apart.
Also data augmentation and other regularization techniques are used to prevent overfitting to specific variations within the training set. Furthermore, model architectures are designed to be robust to these variations, incorporating mechanisms to capture features invariant to changes in pose, lighting, and occlusion. 

In this context, additional modalities such as audio or 3D facial scans can be used to provide complementary information that helps disambiguate facial expressions. For example,  3D facial scans provide a more detailed representation of facial features, which can be used to improve the accuracy of facial expression recognition in cases of bad illumination or occlusion. A growing interest in 3D FER has been driven by the need to improve the accuracy of facial expression recognition in real-world scenarios which are much more affected by these variations. This work focuses on 3D facial expression recognition.\\

In summary, the key challenges faced by FER include the lack of diverse and large-scale datasets, class imbalance, annotation ambiguity, intra-class variability, inter-class similarity. Overcoming these obstacles requires a combination of data augmentation, normalization, regularization, and carefully designed model architectures capable of distinguishing subtle differences between similar emotional expressions. Addressing these issues is crucial for the development of more accurate and reliable FER systems.\\


\subsection{Contribution}
In the following, this work provides a proposal of a deep architecture that is able to deal with multimodal data representation (RGBD) and to learn from the unbalanced dataset. The architecture is based on the state of the art in FER and is designed to be robust to the challenges faced by FER models. Different loss functions are considered evaluating the model's performance on CalD3r, MenD3s \cite{CalD3rMenD3s} and BU3DFE \cite{BU3DFE} dataset in different conditions. Finally, CalD3r, MenD3s\cite{CalD3rMenD3s}, BU3DFE \cite{BU3DFE} and Bosphorus \cite{Bosphorus} datasets are merged to create a larger and more diverse dataset and final evaluation of the model's performance. This is the largest static multimodal FER dataset up to our knowledge.

The main contribution of this work consist in answering to the following questions:
\begin{itemize}
    \item \textbf{How to efficiently combine modalities to improve the performance of a FER model?} \\
    A novel Cross Attention Module is presented
    \item \textbf{How to deal with intra-class variability and inter-class similarity in a multimodal FER model?} Island Loss is used to learn more discriminative features
    \item \textbf{How to create a large and diverse dataset for multimodal FER? And how does a vision transformer based model perform on it?} Multiple datasets are merged to create a larger and more diverse dataset and final evaluation of the model's performance.

\end{itemize}
    


