
\section{CONCLUSIONS}

This work addressed 2D+3D Facial Emotion Recognition (FER) with a hybrid deep network, which uses features extracted from a Convolutional Neural Network (CNN) to train a Vision Transformer Encoder. The proposed network was evaluated in various experimental setups over the CalD3r, MenD3s, BU3DFE, and Bosphorus datasets, along with a new multimodal dataset formed by merging these three datasets. It demonstrated superior performance across the evaluated benchmarks, achieving an accuracy of 91,66\% on BU3DFE, 62,50\% on CalD3r and MenD3s and 68,48\% on the merged dataset. Notably, this work introduces a new benchmark on the BU3DFE dataset by using the entire dataset, providing a more comprehensive evaluation for future research. Additionally, a new multimodal dataset—created by merging the CalD3r, MenD3s, BU3DFE, and Bosphorus datasets—is proposed. This new dataset represents the largest static multimodal dataset for FER to date, combining RGB and depth modalities to better capture geometric information and improve robustness to challenging conditions such as occlusions, lighting variations, and identity-related factors.

The strength of the model lies in actively selecting the most meaningful spatial regions from the features extracted by the CNN, followed by the transformer, which learns a more global representation of the image. Future work could explore introducing spatial attention directly within the backbone networks, rather than only at the final stage, creating an "overloaded" Squeeze-Excite module. This module could apply attention not only over the channel dimension but also across spatial dimensions. Such attention mechanisms could be applied independently in each modality or in a cross-modality setup. This concept could also be implemented in various ways, such as by rotating input features as seen in CBAM.

To further improve model generalization, expanding the dataset with additional resources, such as the 4DFAB dataset, could be beneficial. Larger datasets could also support the use of deeper and larger backbones, potentially improving model performance. Additionally, the model could be adapted to address annotation ambiguity by incorporating a text encoder, as seen in OpenAI's CLIP model, or by using a variant of batch normalization that allows the network to learn to ignore label noise.

A more thorough cross-dataset analysis could be valuable, such as training the model on the global dataset and evaluating its generalization to datasets like 4DFAB.

Finally, given the model's relatively straightforward architecture, it could be integrated into real-time applications for use in real-world scenarios. A device capable of recording RGBD images in real-time could feed these inputs into the model, providing robust predictions even in cases of occlusion or poor lighting conditions


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
